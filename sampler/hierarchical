#!/usr/bin/env python
"""
This module call the hierarchical sampling module to find interesting
points to label
"""

import math
import numpy as np
import pprint
from random import randint, choice, random

from common.configuration import Configuration
from common.util import fatal
from common.tree import node_iterator
from common.fit import residues
from common.constraints import get_tagged_models
from scipy import stats
import common.cart


def leaferror_hoeffding(N, threshold):
    freq_error = 0
    m = N.model[0]
    card = 0
    for d in N.data:
        card += 1
        if abs(d[-1] - m) >= threshold:
            freq_error += 1

    error_probability = (float(freq_error) / float(card))
    assert(0 <= error_probability <= 1)
    R = 1
    confidence = math.log(1.0 / 0.1)
    hoeff = math.sqrt(R * R * confidence / (3 * card))
    return {"error": min(error_probability + hoeff, 1.0),
            "confidence": hoeff,
            "raw_error": error_probability}


def leaferror_variance(N, threshold):
    est_variance = 0
    m = N.model[0]
    card = 0
    for d in N.data:
        card += 1
        est_variance += (d[-1] - m) ** 2

    est_variance = est_variance / float(card-1)
    variance_corr = (card - 1) / stats.chi.ppf(0.05, card - 1) ** 2
    variance_ub = est_variance * variance_corr
    return {"error": variance_ub,
            "confidence": variance_ub - est_variance,
            "raw_error": est_variance}

def leaferror_cov(N, threshold):
    est_variance = 0
    m = N.model[0]
    card = 0
    for d in N.data:
        card += 1
        est_variance += (d[-1]/m - 1) ** 2

    est_variance = est_variance / float(card-1)
    variance_corr = (card - 1) / stats.chi.ppf(0.05, card - 1) ** 2
    variance_ub = est_variance * variance_corr
    return {"error": variance_ub,
            "confidence": variance_ub - est_variance,
            "raw_error": est_variance}


def leaferror_variance_std(N, threshold):
    est_variance = 0
    m = N.model[0]
    card = 0
    for d in N.data:
        card += 1
        est_variance += (d[-1] - m) ** 2

    est_variance = est_variance / float(card-1)
    variance_corr = (card - 1) / stats.chi.ppf(0.05, card - 1) ** 2
    variance_ub = math.sqrt(est_variance * variance_corr)
    est_variance = math.sqrt(est_variance)
    return {"error": variance_ub,
            "confidence": variance_ub - est_variance,
            "raw_error": est_variance}


def leaferror_cov_std(N, threshold):
    est_variance = 0
    m = N.model[0]
    card = 0
    for d in N.data:
        card += 1
        est_variance += (d[-1]/m - 1) ** 2

    est_variance = est_variance / float(card-1)
    variance_corr = (card - 1) / stats.chi.ppf(0.05, card - 1) ** 2
    variance_ub = est_variance * variance_corr
    variance_ub = math.sqrt(est_variance * variance_corr)
    est_variance = math.sqrt(est_variance)
    return {"error": variance_ub,
            "confidence": variance_ub - est_variance,
            "raw_error": est_variance}


def compute_error_pernode(data, T, configuration):
    # fill the tree nodes
    for d in data:
        T.fill(d)

    use_hoeffding = configuration("modules.sampler.params.use_hoeffding",
                              bool,
                              False)
    use_cov = configuration("modules.sampler.params.use_cov",
                              bool,
                              False)
    use_std =  configuration("modules.sampler.params.use_std",
                              bool,
                              False)

    if use_hoeffding:
        print "Using Hoeffding correction"
        # compute threshold
        threshold = np.median(map(abs, residues(data, T)))
        method = leaferror_hoeffding
    elif use_cov:
        print "Using COV Chi^2 correction"
        threshold = None
        if use_std:
            method = leaferror_cov_std
        else:
            method = leaferror_cov
    else:
        print "Using Variance Chi^2 correction"
        threshold = None
        if use_std:
            method = leaferror_variance_std
        else:
            method = leaferror_variance

    # compute the error on each node
    for N in node_iterator(T):
        N.error = method(N, threshold)


def compute_size(constraint):
    size = 1

    for v in constraint.itervalues():
        if not isinstance(v, dict):
            size *= len(v)
        else:
            size *= v["max"] - v["min"]
    assert(size >= 0)
    return size


def posargmin(x):
    pos = [k for k in x if k > 0]
    return x.index(min(pos))


def find_sampling_distribution(configuration, T, samplesize):
    factors = configuration("factors")
    ponderate = configuration("modules.sampler.params.ponderate_by_size",
                              bool,
                              True)

    cons_raw = get_tagged_models(factors,
                                 T)
    if ponderate:
        print "Ponderating by size"
        prob = [c["error"]["error"] * compute_size(c["constraint"])
                for c in cons_raw]
    else:
        print "Not ponderating by size"
        prob = [c["error"]["error"] for c in cons_raw]

    totalp = sum(prob)
    proportions = [p * samplesize / totalp for p in prob]
    rounded = map(int, map(math.ceil, proportions))

    for k, p in zip(cons_raw, proportions):
        k["raw_proportion"] = p

    # remove rounding error
    while (sum(rounded) - samplesize):
        p = posargmin(proportions)
        rounded[p] -= 1
        proportions[p] -= 1

    proportions = rounded

    for k, p in zip(cons_raw, proportions):
        k["sampling_proportion"] = p

    # Print tree regions to debug
    pprint.pprint(cons_raw)

    # XXX This section was used to generate a figure for
    # an article, remove it before any release.
    #
    #import os
    #f=open(os.path.join(configuration("output_directory"),
    #       "hierarchical.dump"), "w")
    #f.write("mi ma variance ub mean\n")
    #for k in cons_raw:
    #    mi =float(k["constraint"]["x"]["min"])
    #    ma =float(k["constraint"]["x"]["max"])
    #    var = math.sqrt(k["error"]["raw_error"])
    #    ub = math.sqrt(k["error"]["error"])
    #    mean = k["model"][0]
    #    f.write("%s %s %s %s %s\n"%(mi,ma, var, ub, mean))
    #f.close()

    # ensure we satisfy exactly the samplesize
    assert(sum(proportions) == samplesize)
    # ensure there are no negative proportions
    assert(not [p for p in proportions if p < 0])
    return zip(proportions, [c["constraint"] for c in cons_raw])


def draw_point(factors, constraints):
    p = []
    for f in factors:
        c = constraints[f["name"]]
        if f["type"] == "integer":
            g = randint(int(c["min"]),
                        int(c["max"]))
            p.append(g)
        elif f["type"] == "float":
            g = random() * (c["max"] - c["min"]) + c["min"]
            p.append(g)
        elif f["type"] == "categorical":
            g = f["values"].index(choice(c))
            p.append(g)
        else:
            fatal("Wrong factor type: <{0}>"
                  .format(f["type"]))
    return tuple(p)


def generate_possible_points(factors, constraints, labelled, n):
    # Prepare a list of generators that generate all possible
    # factors satisfying constraints
    count = 0
    tries = 0
    # We draw a random point until we find a point never selected
    # or we have done more than MAX_TRIES
    MAX_TRIES = 50
    while(count < n):
        p = draw_point(factors, constraints)
        if p in labelled and tries < MAX_TRIES:
            tries += 1
            continue
        else:
            tries = 0
            count += 1
            yield p


def predict(configuration, input_file, output_file, n, cp):
    print "using cut cp = {0}".format(cp)
    # build partition tree
    T = common.cart.build_tree(configuration, cp, input_file)

    # read the labelled points
    labelled = np.genfromtxt(input_file)

    # when there is only one observation we have to
    # reshape the data to say it is multidimensionnal
    if len(labelled.shape) == 1:
        labelled = labelled.reshape(1, labelled.shape[0])

    # compute the leaf error
    compute_error_pernode(labelled, T, configuration)

    # compute sampling distribution
    distribution = find_sampling_distribution(configuration, T, n)

    # open a file to write suggested points
    of = open(output_file, "w")

    # take all the labelled points and create a set
    already_labelled = set()
    for v in labelled[:,:-1]:
        already_labelled.add(tuple(v))

    for size, cons in distribution:
        for p in generate_possible_points(configuration["factors"],
                                          cons,
                                          already_labelled,
                                          size):
            already_labelled.add(p)
            of.write(" ".join(map(str, p)) + "\n")
    of.close()

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(
        description="Using the currently labelled points,"
        " detects the more imprecise zones and draws points from them")
    parser.add_argument('configuration')
    parser.add_argument('input_file')
    parser.add_argument('output_file')
    args = parser.parse_args()
    conf = Configuration(args.configuration)
    predict(
        conf,
        args.input_file,
        args.output_file,
        conf("modules.sampler.params.n", int),
        conf("modules.sampler.params.cp", float, 0.0000001))
